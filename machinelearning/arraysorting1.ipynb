{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfd7da-da19-4e78-8879-d12f714210d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35558d-ca1c-473d-8c0d-aa191b0fb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be an environment of an array 4*1 (eg. [4,3,2,1] or [1,2,4,3]\n",
    "class ArrayEnv(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.game_array_size = 100\n",
    "\n",
    "        self.action_space = Discrete(6) #6 possible types of swaps (01,02,03,12,13,23)\n",
    "\n",
    "        high = np.array([1000] * 4)\n",
    "        low = np.array([0] * 4)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(4))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "        #print(self.state)\n",
    "        #get the two values to swap\n",
    "        if action == 0:\n",
    "            x_indice = 0\n",
    "            y_indice = 1\n",
    "            \n",
    "        elif action == 1:\n",
    "            x_indice = 0\n",
    "            y_indice = 2\n",
    "            \n",
    "        elif action == 2:\n",
    "            x_indice = 0\n",
    "            y_indice = 3\n",
    "            \n",
    "        elif action == 3:\n",
    "            x_indice = 1\n",
    "            y_indice = 2\n",
    "            \n",
    "        elif action == 4:\n",
    "            x_indice = 1\n",
    "            y_indice = 3\n",
    "            \n",
    "        elif action == 5:\n",
    "            x_indice = 2\n",
    "            y_indice = 3\n",
    "        else:\n",
    "            print(\"NO\")\n",
    "        \n",
    "\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/4)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(4))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e58fb-82fd-4380-afd5-642fb02cdca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second attempt intended for multiple array lengths\n",
    "class ArrayEnv2(Env):\n",
    "    \n",
    "    def __init__(self, game_size):\n",
    "\n",
    "        self.game_array_size = game_size\n",
    "\n",
    "        self.action_space = MultiDiscrete([self.game_array_size, self.game_array_size]) #10 possible xs and 10 possible ys\n",
    "\n",
    "        high = np.array([1000] * self.game_array_size)\n",
    "        low = np.array([0] * self.game_array_size)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "\n",
    "        \n",
    "        x_indice = action[0]\n",
    "        y_indice = action[1]\n",
    "        #print(\"From: {}\\tX: {}\\tY: {}\".format(action, x_indice, y_indice))\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/self.game_array_size)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "        else:\n",
    "            reward = -100 #offending line... maybe fix this\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277fa21-d36a-40c2-b25e-0dd09eab8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this incorporates a time reward for speedier solves\n",
    "class ArrayEnv3(Env):\n",
    "    \n",
    "    def __init__(self, game_size):\n",
    "    \n",
    "        self.steps = 0\n",
    "        self.game_array_size = game_size\n",
    "\n",
    "        self.action_space = MultiDiscrete([self.game_array_size, self.game_array_size]) #10 possible xs and 10 possible ys\n",
    "\n",
    "        high = np.array([1000] * self.game_array_size)\n",
    "        low = np.array([0] * self.game_array_size)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "        self.steps+=1 #keeps track of how many steps have been completed\n",
    "        \n",
    "        x_indice = action[0]\n",
    "        y_indice = action[1]\n",
    "        #print(\"From: {}\\tX: {}\\tY: {}\".format(action, x_indice, y_indice))\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/self.game_array_size)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "\n",
    "        elif x_indice > y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/self.game_array_size)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original < y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "        \n",
    "        else:\n",
    "            reward = -100 #swapping the same array place, useless action\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        \n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            #add in extra reward based off of time it took to solve the array.\n",
    "            if self.steps <= self.game_array_size:\n",
    "                reward+=100\n",
    "            else:\n",
    "                reward += 100/self.steps\n",
    "            \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383357f-f4b4-48e7-ad56-5af108262965",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./PPO_array_6/PPO_4\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 741           |\n",
      "|    ep_rew_mean          | -4.3e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 241           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 33            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1098135e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.58         |\n",
      "|    explained_variance   | 0.000132      |\n",
      "|    learning_rate        | 2e-05         |\n",
      "|    loss                 | 4.59e+05      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000293     |\n",
      "|    value_loss           | 9.3e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 752           |\n",
      "|    ep_rew_mean          | -4.37e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 220           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 74            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3726257e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.58         |\n",
      "|    explained_variance   | 0.000743      |\n",
      "|    learning_rate        | 2e-05         |\n",
      "|    loss                 | 4.56e+05      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000347     |\n",
      "|    value_loss           | 9.1e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 849           |\n",
      "|    ep_rew_mean          | -4.95e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 204           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 120           |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7095055e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.58         |\n",
      "|    explained_variance   | 8.31e-05      |\n",
      "|    learning_rate        | 2e-05         |\n",
      "|    loss                 | 4.56e+05      |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000442     |\n",
      "|    value_loss           | 9.47e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 940           |\n",
      "|    ep_rew_mean          | -5.49e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 205           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 159           |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4221325e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.58         |\n",
      "|    explained_variance   | 0.00028       |\n",
      "|    learning_rate        | 2e-05         |\n",
      "|    loss                 | 4.79e+05      |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.000358     |\n",
      "|    value_loss           | 9.22e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 870          |\n",
      "|    ep_rew_mean          | -5.07e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 192          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 212          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.705852e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.58        |\n",
      "|    explained_variance   | -9.18e-06    |\n",
      "|    learning_rate        | 2e-05        |\n",
      "|    loss                 | 4.49e+05     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000537    |\n",
      "|    value_loss           | 9.35e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 872          |\n",
      "|    ep_rew_mean          | -5.08e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 194          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 253          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.584784e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.58        |\n",
      "|    explained_variance   | -1.31e-05    |\n",
      "|    learning_rate        | 2e-05        |\n",
      "|    loss                 | 4.79e+05     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.00046     |\n",
      "|    value_loss           | 9.15e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 894           |\n",
      "|    ep_rew_mean          | -5.19e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 195           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 293           |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.9519015e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.58         |\n",
      "|    explained_variance   | 7.21e-06      |\n",
      "|    learning_rate        | 2e-05         |\n",
      "|    loss                 | 4.65e+05      |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.000527     |\n",
      "|    value_loss           | 9.07e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 831           |\n",
      "|    ep_rew_mean          | -4.83e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 190           |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 344           |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6838417e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.58         |\n",
      "|    explained_variance   | -1.54e-05     |\n",
      "|    learning_rate        | 2e-05         |\n",
      "|    loss                 | 4.63e+05      |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.000408     |\n",
      "|    value_loss           | 8.96e+05      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#del env\n",
    "#del model\n",
    "env = ArrayEnv3(6)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_6/\", learning_rate = 0.00002)\n",
    "\n",
    "#model = PPO.load(\"test4-01\")\n",
    "#model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=15000000, log_interval=4)\n",
    "\n",
    "model.save(\"test6-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48cf59b5-dc57-46d6-a3e4-c576a3ccdca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test5-01\t avg. moves: 4.149\t % Neg: 3.16\n",
      "For test5-02\t avg. moves: 4.4274\t % Neg: 6.569999999999999\n",
      "For test5-03\t avg. moves: 4.1715\t % Neg: 3.36\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range (1, 4):\n",
    "    #del env\n",
    "    #del model\n",
    "    model = PPO.load(\"test5-0{}.zip\".format(i))\n",
    "    env = ArrayEnv3(5)\n",
    "\n",
    "    obs = env.reset()\n",
    "    episodes = 1000000\n",
    "    negatives = 0\n",
    "    total_moves = 0\n",
    "    for episode in range(1, episodes+1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0 \n",
    "        #print(\"--- original array ---\")\n",
    "        #env.render()\n",
    "        #print(\"--- beginning sort ---\")\n",
    "        moves = 0\n",
    "        #print(\"|\", end=\"\")\n",
    "        while not done:\n",
    "            total_moves+=1\n",
    "            moves+=1\n",
    "            action, _states = model.predict(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "            #env.render()\n",
    "        if score < 0:\n",
    "            negatives+=1\n",
    "        #print(\"Episode: {} \\tMoves: {}\\tScore: {}\".format(episode, moves, score))\n",
    "\n",
    "    print(\"For test5-0{}\\t avg. moves: {}\\t % Neg: {}\".format(i, (total_moves/episodes), (negatives/episodes)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef491c0d-2a64-423a-a46f-2f26c11a808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del model\n",
    "env = ArrayEnv3(6)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_6/\", learning_rate = 0.00001)\n",
    "\n",
    "#model = PPO.load(\"test4-01\")\n",
    "#model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=15000000, log_interval=4)\n",
    "\n",
    "model.save(\"test6-05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337d494-aaff-4ffb-a7ae-e09465867b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del model\n",
    "env = ArrayEnv3(7)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_7/\", learning_rate = 0.00002)\n",
    "\n",
    "#model = PPO.load(\"test4-01\")\n",
    "#model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=15000000, log_interval=4)\n",
    "\n",
    "model.save(\"test7-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08419134-534b-4a5a-9ae7-bf9c7060ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del model\n",
    "env = ArrayEnv3(5)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_5/\", learning_rate = 0.00002)\n",
    "model.learn(total_timesteps=10000000, log_interval=4)\n",
    "\n",
    "model.save(\"test5-06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b092d-bc2f-4a76-a807-5c92bdc60371",
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del model\n",
    "env = ArrayEnv3(5)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_5/\", learning_rate = 0.00001)\n",
    "model.learn(total_timesteps=15000000, log_interval=4)\n",
    "\n",
    "model.save(\"test5-06\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a16dd4-00bb-4c02-9ac9-aa16009a3c2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
