{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfd7da-da19-4e78-8879-d12f714210d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35558d-ca1c-473d-8c0d-aa191b0fb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be an environment of an array 4*1 (eg. [4,3,2,1] or [1,2,4,3]\n",
    "class ArrayEnv(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.game_array_size = 100\n",
    "\n",
    "        self.action_space = Discrete(6) #6 possible types of swaps (01,02,03,12,13,23)\n",
    "\n",
    "        high = np.array([1000] * 4)\n",
    "        low = np.array([0] * 4)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(4))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "        #print(self.state)\n",
    "        #get the two values to swap\n",
    "        if action == 0:\n",
    "            x_indice = 0\n",
    "            y_indice = 1\n",
    "            \n",
    "        elif action == 1:\n",
    "            x_indice = 0\n",
    "            y_indice = 2\n",
    "            \n",
    "        elif action == 2:\n",
    "            x_indice = 0\n",
    "            y_indice = 3\n",
    "            \n",
    "        elif action == 3:\n",
    "            x_indice = 1\n",
    "            y_indice = 2\n",
    "            \n",
    "        elif action == 4:\n",
    "            x_indice = 1\n",
    "            y_indice = 3\n",
    "            \n",
    "        elif action == 5:\n",
    "            x_indice = 2\n",
    "            y_indice = 3\n",
    "        else:\n",
    "            print(\"NO\")\n",
    "        \n",
    "\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/4)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(4))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e58fb-82fd-4380-afd5-642fb02cdca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second attempt intended for multiple array lengths\n",
    "class ArrayEnv2(Env):\n",
    "    \n",
    "    def __init__(self, game_size):\n",
    "\n",
    "        self.game_array_size = game_size\n",
    "\n",
    "        self.action_space = MultiDiscrete([self.game_array_size, self.game_array_size]) #10 possible xs and 10 possible ys\n",
    "\n",
    "        high = np.array([1000] * self.game_array_size)\n",
    "        low = np.array([0] * self.game_array_size)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "\n",
    "        \n",
    "        x_indice = action[0]\n",
    "        y_indice = action[1]\n",
    "        #print(\"From: {}\\tX: {}\\tY: {}\".format(action, x_indice, y_indice))\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/self.game_array_size)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "        else:\n",
    "            reward = -100 #offending line... maybe fix this\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277fa21-d36a-40c2-b25e-0dd09eab8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this incorporates a time reward for speedier solves\n",
    "class ArrayEnv3(Env):\n",
    "    \n",
    "    def __init__(self, game_size):\n",
    "    \n",
    "        self.steps = 0\n",
    "        self.game_array_size = game_size\n",
    "\n",
    "        self.action_space = MultiDiscrete([self.game_array_size, self.game_array_size]) #10 possible xs and 10 possible ys\n",
    "\n",
    "        high = np.array([1000] * self.game_array_size)\n",
    "        low = np.array([0] * self.game_array_size)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "        self.steps+=1 #keeps track of how many steps have been completed\n",
    "        \n",
    "        x_indice = action[0]\n",
    "        y_indice = action[1]\n",
    "        #print(\"From: {}\\tX: {}\\tY: {}\".format(action, x_indice, y_indice))\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/self.game_array_size)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "\n",
    "        elif x_indice > y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/self.game_array_size)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original < y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "        \n",
    "        else:\n",
    "            reward = -100 #swapping the same array place, useless action\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        \n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            #add in extra reward based off of time it took to solve the array.\n",
    "            if self.steps <= self.game_array_size:\n",
    "                reward+=100\n",
    "            else:\n",
    "                reward += 100/self.steps\n",
    "            \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383357f-f4b4-48e7-ad56-5af108262965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del env\n",
    "#del model\n",
    "env = ArrayEnv3(5)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_5/\", learning_rate = 0.00002)\n",
    "\n",
    "#model = PPO.load(\"test4-01\")\n",
    "#model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=5000000, log_interval=4)\n",
    "\n",
    "model.save(\"test5-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48cf59b5-dc57-46d6-a3e4-c576a3ccdca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test5-01\t avg. moves: 4.22\t % Neg: 4.0\n",
      "For test5-02\t avg. moves: 4.2\t % Neg: 8.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range (1, 3):\n",
    "    #del env\n",
    "    #del model\n",
    "    model = PPO.load(\"test5-0{}.zip\".format(i))\n",
    "    env = ArrayEnv3(5)\n",
    "\n",
    "    obs = env.reset()\n",
    "    episodes = 100\n",
    "    negatives = 0\n",
    "    total_moves = 0\n",
    "    for episode in range(1, episodes+1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0 \n",
    "        #print(\"--- original array ---\")\n",
    "        #env.render()\n",
    "        #print(\"--- beginning sort ---\")\n",
    "        moves = 0\n",
    "        #print(\"|\", end=\"\")\n",
    "        while not done:\n",
    "            total_moves+=1\n",
    "            moves+=1\n",
    "            action, _states = model.predict(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "            #env.render()\n",
    "        if score < 0:\n",
    "            negatives+=1\n",
    "        #print(\"Episode: {} \\tMoves: {}\\tScore: {}\".format(episode, moves, score))\n",
    "\n",
    "    print(\"For test5-0{}\\t avg. moves: {}\\t % Neg: {}\".format(i, (total_moves/episodes), (negatives/episodes)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef491c0d-2a64-423a-a46f-2f26c11a808f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./PPO_array_5/PPO_3\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-d18b3b4c2ce7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#model.set_env(env)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test5-02\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m             \u001b[0meval_log_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meval_log_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m             \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         )\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stable_baselines3/common/on_policy_algorithm.py\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, eval_env, eval_freq, n_eval_episodes, tb_log_name, eval_log_path, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_timesteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_training_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stable_baselines3/ppo/ppo.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_noise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mentropy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrollout_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                 \u001b[0;31m# Normalize advantage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mevaluate_actions\u001b[0;34m(self, obs, actions)\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# Preprocess the observation if needed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m         \u001b[0mlatent_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_vf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmlp_extractor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m         \u001b[0mdistribution\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_action_dist_from_latent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m         \u001b[0mlog_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdistribution\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/stable_baselines3/common/torch_layers.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \"\"\"\n\u001b[1;32m    227\u001b[0m         \u001b[0mshared_latent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshared_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshared_latent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward_actor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mSiLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "del env\n",
    "del model\n",
    "env = ArrayEnv3(5)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_5/\", learning_rate = 0.00001)\n",
    "\n",
    "#model = PPO.load(\"test4-01\")\n",
    "#model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=5000000, log_interval=4)\n",
    "\n",
    "model.save(\"test5-02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337d494-aaff-4ffb-a7ae-e09465867b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del model\n",
    "env = ArrayEnv3(5)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_5/\", learning_rate = 0.00001)\n",
    "\n",
    "#model = PPO.load(\"test4-01\")\n",
    "#model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=10000000, log_interval=4)\n",
    "\n",
    "model.save(\"test5-03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08419134-534b-4a5a-9ae7-bf9c7060ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del model\n",
    "env = ArrayEnv2(5)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_5/\", learning_rate = 0.000005)\n",
    "model.learn(total_timesteps=15000000, log_interval=4)\n",
    "\n",
    "model.save(\"test5-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b092d-bc2f-4a76-a807-5c92bdc60371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
