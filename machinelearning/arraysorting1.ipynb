{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddfd7da-da19-4e78-8879-d12f714210d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, MultiDiscrete\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c35558d-ca1c-473d-8c0d-aa191b0fb8ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be an environment of an array 4*1 (eg. [4,3,2,1] or [1,2,4,3]\n",
    "class ArrayEnv(Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.game_array_size = 100\n",
    "\n",
    "        self.action_space = Discrete(6) #6 possible types of swaps (01,02,03,12,13,23)\n",
    "\n",
    "        high = np.array([1000] * 4)\n",
    "        low = np.array([0] * 4)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(4))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "        #print(self.state)\n",
    "        #get the two values to swap\n",
    "        if action == 0:\n",
    "            x_indice = 0\n",
    "            y_indice = 1\n",
    "            \n",
    "        elif action == 1:\n",
    "            x_indice = 0\n",
    "            y_indice = 2\n",
    "            \n",
    "        elif action == 2:\n",
    "            x_indice = 0\n",
    "            y_indice = 3\n",
    "            \n",
    "        elif action == 3:\n",
    "            x_indice = 1\n",
    "            y_indice = 2\n",
    "            \n",
    "        elif action == 4:\n",
    "            x_indice = 1\n",
    "            y_indice = 3\n",
    "            \n",
    "        elif action == 5:\n",
    "            x_indice = 2\n",
    "            y_indice = 3\n",
    "        else:\n",
    "            print(\"NO\")\n",
    "        \n",
    "\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/4)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(4))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8e58fb-82fd-4380-afd5-642fb02cdca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#second attempt intended for multiple array lengths\n",
    "class ArrayEnv2(Env):\n",
    "    \n",
    "    def __init__(self, game_size):\n",
    "\n",
    "        self.game_array_size = game_size\n",
    "\n",
    "        self.action_space = MultiDiscrete([self.game_array_size, self.game_array_size]) #10 possible xs and 10 possible ys\n",
    "\n",
    "        high = np.array([1000] * self.game_array_size)\n",
    "        low = np.array([0] * self.game_array_size)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "\n",
    "        \n",
    "        x_indice = action[0]\n",
    "        y_indice = action[1]\n",
    "        #print(\"From: {}\\tX: {}\\tY: {}\".format(action, x_indice, y_indice))\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/self.game_array_size)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "        else:\n",
    "            reward = -100 #offending line... maybe fix this\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3277fa21-d36a-40c2-b25e-0dd09eab8865",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this incorporates a time reward for speedier solves\n",
    "class ArrayEnv3(Env):\n",
    "    \n",
    "    def __init__(self, game_size):\n",
    "    \n",
    "        self.steps = 0\n",
    "        self.game_array_size = game_size\n",
    "\n",
    "        self.action_space = MultiDiscrete([self.game_array_size, self.game_array_size]) #10 possible xs and 10 possible ys\n",
    "\n",
    "        high = np.array([1000] * self.game_array_size)\n",
    "        low = np.array([0] * self.game_array_size)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "        self.steps+=1 #keeps track of how many steps have been completed\n",
    "        \n",
    "        x_indice = action[0]\n",
    "        y_indice = action[1]\n",
    "        #print(\"From: {}\\tX: {}\\tY: {}\".format(action, x_indice, y_indice))\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/self.game_array_size)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "\n",
    "        elif x_indice > y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/self.game_array_size)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original < y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "        \n",
    "        else:\n",
    "            reward = -100 #swapping the same array place, useless action\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        \n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            #add in extra reward based off of time it took to solve the array.\n",
    "            if self.steps <= self.game_array_size:\n",
    "                reward+=100\n",
    "            else:\n",
    "                reward += 100/self.steps\n",
    "            \n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(self.game_array_size))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4383357f-f4b4-48e7-ad56-5af108262965",
   "metadata": {},
   "outputs": [],
   "source": [
    "#del env\n",
    "#del model\n",
    "env = ArrayEnv3(5)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_5/\", learning_rate = 0.00002)\n",
    "\n",
    "#model = PPO.load(\"test4-01\")\n",
    "#model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=5000000, log_interval=4)\n",
    "\n",
    "model.save(\"test5-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48cf59b5-dc57-46d6-a3e4-c576a3ccdca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For test5-01\t avg. moves: 4.107\t % Neg: 2.5\n",
      "For test5-02\t avg. moves: 4.301\t % Neg: 5.6000000000000005\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range (1, 3):\n",
    "    #del env\n",
    "    #del model\n",
    "    model = PPO.load(\"test5-0{}.zip\".format(i))\n",
    "    env = ArrayEnv3(5)\n",
    "\n",
    "    obs = env.reset()\n",
    "    episodes = 1000\n",
    "    negatives = 0\n",
    "    total_moves = 0\n",
    "    for episode in range(1, episodes+1):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        score = 0 \n",
    "        #print(\"--- original array ---\")\n",
    "        #env.render()\n",
    "        #print(\"--- beginning sort ---\")\n",
    "        moves = 0\n",
    "        #print(\"|\", end=\"\")\n",
    "        while not done:\n",
    "            total_moves+=1\n",
    "            moves+=1\n",
    "            action, _states = model.predict(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score+=reward\n",
    "            #env.render()\n",
    "        if score < 0:\n",
    "            negatives+=1\n",
    "        #print(\"Episode: {} \\tMoves: {}\\tScore: {}\".format(episode, moves, score))\n",
    "\n",
    "    print(\"For test5-0{}\\t avg. moves: {}\\t % Neg: {}\".format(i, (total_moves/episodes), (negatives/episodes)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef491c0d-2a64-423a-a46f-2f26c11a808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del model\n",
    "env = ArrayEnv3(5)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_5/\", learning_rate = 0.00001)\n",
    "\n",
    "#model = PPO.load(\"test4-01\")\n",
    "#model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=5000000, log_interval=4)\n",
    "\n",
    "model.save(\"test5-02\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1337d494-aaff-4ffb-a7ae-e09465867b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Logging to ./PPO_array_5/PPO_3\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 175           |\n",
      "|    ep_rew_mean          | -1.03e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 633           |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 12            |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8025825e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | 0.000184      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.39e+05      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.000145     |\n",
      "|    value_loss           | 8.4e+05       |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 167           |\n",
      "|    ep_rew_mean          | -9.91e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 621           |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 26            |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3869292e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -0.000487     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.26e+05      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.000164     |\n",
      "|    value_loss           | 8.53e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 159           |\n",
      "|    ep_rew_mean          | -9.38e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 620           |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 39            |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.8549264e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -0.000504     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.45e+05      |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000179     |\n",
      "|    value_loss           | 8.67e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 172           |\n",
      "|    ep_rew_mean          | -1.01e+04     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 604           |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 54            |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0023297e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -0.000193     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.6e+05       |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.000178     |\n",
      "|    value_loss           | 8.69e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 168          |\n",
      "|    ep_rew_mean          | -9.91e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 614          |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 66           |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.432717e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | -0.000105    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.06e+05     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000158    |\n",
      "|    value_loss           | 8.46e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 179          |\n",
      "|    ep_rew_mean          | -1.06e+04    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 624          |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 78           |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.689943e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | -0.00044     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.44e+05     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000108    |\n",
      "|    value_loss           | 8.72e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 186           |\n",
      "|    ep_rew_mean          | -1.1e+04      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 629           |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 91            |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.0898386e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -4.49e-05     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.75e+05      |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -0.000206     |\n",
      "|    value_loss           | 8.89e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 138           |\n",
      "|    ep_rew_mean          | -8.26e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 636           |\n",
      "|    iterations           | 32            |\n",
      "|    time_elapsed         | 102           |\n",
      "|    total_timesteps      | 65536         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.5038254e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -3.02e-05     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.14e+05      |\n",
      "|    n_updates            | 310           |\n",
      "|    policy_gradient_loss | -0.000209     |\n",
      "|    value_loss           | 8.84e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 147          |\n",
      "|    ep_rew_mean          | -8.71e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 640          |\n",
      "|    iterations           | 36           |\n",
      "|    time_elapsed         | 115          |\n",
      "|    total_timesteps      | 73728        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.426904e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | -4.28e-05    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.17e+05     |\n",
      "|    n_updates            | 350          |\n",
      "|    policy_gradient_loss | -0.00019     |\n",
      "|    value_loss           | 8.48e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 153         |\n",
      "|    ep_rew_mean          | -9.04e+03   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 647         |\n",
      "|    iterations           | 40          |\n",
      "|    time_elapsed         | 126         |\n",
      "|    total_timesteps      | 81920       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.82108e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.22       |\n",
      "|    explained_variance   | -5.96e-07   |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 4.34e+05    |\n",
      "|    n_updates            | 390         |\n",
      "|    policy_gradient_loss | -0.000213   |\n",
      "|    value_loss           | 8.72e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 145           |\n",
      "|    ep_rew_mean          | -8.55e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 652           |\n",
      "|    iterations           | 44            |\n",
      "|    time_elapsed         | 138           |\n",
      "|    total_timesteps      | 90112         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7568312e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | 1.24e-05      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.32e+05      |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -0.000125     |\n",
      "|    value_loss           | 8.82e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 136          |\n",
      "|    ep_rew_mean          | -7.97e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 655          |\n",
      "|    iterations           | 48           |\n",
      "|    time_elapsed         | 149          |\n",
      "|    total_timesteps      | 98304        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.916708e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | -3.09e-05    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.69e+05     |\n",
      "|    n_updates            | 470          |\n",
      "|    policy_gradient_loss | -0.000245    |\n",
      "|    value_loss           | 7.63e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 137           |\n",
      "|    ep_rew_mean          | -8.02e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 658           |\n",
      "|    iterations           | 52            |\n",
      "|    time_elapsed         | 161           |\n",
      "|    total_timesteps      | 106496        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.6102505e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | 1.28e-05      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 3.93e+05      |\n",
      "|    n_updates            | 510           |\n",
      "|    policy_gradient_loss | -0.000191     |\n",
      "|    value_loss           | 8.57e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 140          |\n",
      "|    ep_rew_mean          | -8.28e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 661          |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 173          |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.371003e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | 1.91e-06     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.37e+05     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -0.000208    |\n",
      "|    value_loss           | 8.57e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 147           |\n",
      "|    ep_rew_mean          | -8.69e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 660           |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 185           |\n",
      "|    total_timesteps      | 122880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4593395e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -0.00012      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.31e+05      |\n",
      "|    n_updates            | 590           |\n",
      "|    policy_gradient_loss | -0.000185     |\n",
      "|    value_loss           | 8.31e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 154          |\n",
      "|    ep_rew_mean          | -9.08e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 658          |\n",
      "|    iterations           | 64           |\n",
      "|    time_elapsed         | 198          |\n",
      "|    total_timesteps      | 131072       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.220674e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | -7.27e-06    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.5e+05      |\n",
      "|    n_updates            | 630          |\n",
      "|    policy_gradient_loss | -0.000213    |\n",
      "|    value_loss           | 9.2e+05      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 145           |\n",
      "|    ep_rew_mean          | -8.6e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 658           |\n",
      "|    iterations           | 68            |\n",
      "|    time_elapsed         | 211           |\n",
      "|    total_timesteps      | 139264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5142813e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | 3.58e-07      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.53e+05      |\n",
      "|    n_updates            | 670           |\n",
      "|    policy_gradient_loss | -0.00024      |\n",
      "|    value_loss           | 8.67e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 123          |\n",
      "|    ep_rew_mean          | -7.27e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 660          |\n",
      "|    iterations           | 72           |\n",
      "|    time_elapsed         | 223          |\n",
      "|    total_timesteps      | 147456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.934605e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | -1.19e-07    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.92e+05     |\n",
      "|    n_updates            | 710          |\n",
      "|    policy_gradient_loss | -0.000169    |\n",
      "|    value_loss           | 7.73e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 125          |\n",
      "|    ep_rew_mean          | -7.37e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 659          |\n",
      "|    iterations           | 76           |\n",
      "|    time_elapsed         | 235          |\n",
      "|    total_timesteps      | 155648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.888359e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | 4.77e-07     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.37e+05     |\n",
      "|    n_updates            | 750          |\n",
      "|    policy_gradient_loss | -0.000199    |\n",
      "|    value_loss           | 8.8e+05      |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 154          |\n",
      "|    ep_rew_mean          | -9.11e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 657          |\n",
      "|    iterations           | 80           |\n",
      "|    time_elapsed         | 249          |\n",
      "|    total_timesteps      | 163840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.360549e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | -4.77e-07    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.29e+05     |\n",
      "|    n_updates            | 790          |\n",
      "|    policy_gradient_loss | -0.000305    |\n",
      "|    value_loss           | 8.61e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 158           |\n",
      "|    ep_rew_mean          | -9.33e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 656           |\n",
      "|    iterations           | 84            |\n",
      "|    time_elapsed         | 261           |\n",
      "|    total_timesteps      | 172032        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.1221515e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -2.26e-06     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.07e+05      |\n",
      "|    n_updates            | 830           |\n",
      "|    policy_gradient_loss | -0.00025      |\n",
      "|    value_loss           | 7.94e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 149          |\n",
      "|    ep_rew_mean          | -8.78e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 654          |\n",
      "|    iterations           | 88           |\n",
      "|    time_elapsed         | 275          |\n",
      "|    total_timesteps      | 180224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.968482e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | -2.38e-07    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.65e+05     |\n",
      "|    n_updates            | 870          |\n",
      "|    policy_gradient_loss | -0.000201    |\n",
      "|    value_loss           | 7.99e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 149          |\n",
      "|    ep_rew_mean          | -8.78e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 92           |\n",
      "|    time_elapsed         | 288          |\n",
      "|    total_timesteps      | 188416       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.932024e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | 1.19e-07     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.53e+05     |\n",
      "|    n_updates            | 910          |\n",
      "|    policy_gradient_loss | -0.000241    |\n",
      "|    value_loss           | 8.27e+05     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 150          |\n",
      "|    ep_rew_mean          | -8.78e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 96           |\n",
      "|    time_elapsed         | 301          |\n",
      "|    total_timesteps      | 196608       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.787967e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | 4.17e-07     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.64e+05     |\n",
      "|    n_updates            | 950          |\n",
      "|    policy_gradient_loss | -0.000288    |\n",
      "|    value_loss           | 8.46e+05     |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 122         |\n",
      "|    ep_rew_mean          | -7.1e+03    |\n",
      "| time/                   |             |\n",
      "|    fps                  | 653         |\n",
      "|    iterations           | 100         |\n",
      "|    time_elapsed         | 313         |\n",
      "|    total_timesteps      | 204800      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 7.54185e-06 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -3.22       |\n",
      "|    explained_variance   | 1.79e-07    |\n",
      "|    learning_rate        | 1e-05       |\n",
      "|    loss                 | 4.02e+05    |\n",
      "|    n_updates            | 990         |\n",
      "|    policy_gradient_loss | -0.000273   |\n",
      "|    value_loss           | 7.57e+05    |\n",
      "-----------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 115           |\n",
      "|    ep_rew_mean          | -6.7e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 653           |\n",
      "|    iterations           | 104           |\n",
      "|    time_elapsed         | 325           |\n",
      "|    total_timesteps      | 212992        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8865837e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | 1.19e-07      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 3.92e+05      |\n",
      "|    n_updates            | 1030          |\n",
      "|    policy_gradient_loss | -0.000183     |\n",
      "|    value_loss           | 8.53e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 135          |\n",
      "|    ep_rew_mean          | -7.97e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 652          |\n",
      "|    iterations           | 108          |\n",
      "|    time_elapsed         | 339          |\n",
      "|    total_timesteps      | 221184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.413153e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | -9.54e-07    |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.27e+05     |\n",
      "|    n_updates            | 1070         |\n",
      "|    policy_gradient_loss | -0.000306    |\n",
      "|    value_loss           | 9.01e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 144           |\n",
      "|    ep_rew_mean          | -8.52e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 650           |\n",
      "|    iterations           | 112           |\n",
      "|    time_elapsed         | 352           |\n",
      "|    total_timesteps      | 229376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0986143e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -1.19e-07     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.23e+05      |\n",
      "|    n_updates            | 1110          |\n",
      "|    policy_gradient_loss | -0.000298     |\n",
      "|    value_loss           | 8.79e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 145           |\n",
      "|    ep_rew_mean          | -8.58e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 647           |\n",
      "|    iterations           | 116           |\n",
      "|    time_elapsed         | 366           |\n",
      "|    total_timesteps      | 237568        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6515994e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.26e+05      |\n",
      "|    n_updates            | 1150          |\n",
      "|    policy_gradient_loss | -0.000176     |\n",
      "|    value_loss           | 8.48e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 161          |\n",
      "|    ep_rew_mean          | -9.53e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 646          |\n",
      "|    iterations           | 120          |\n",
      "|    time_elapsed         | 380          |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.298432e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 3.96e+05     |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -0.000212    |\n",
      "|    value_loss           | 8.75e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 158           |\n",
      "|    ep_rew_mean          | -9.33e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 646           |\n",
      "|    iterations           | 124           |\n",
      "|    time_elapsed         | 392           |\n",
      "|    total_timesteps      | 253952        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1785392e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | 2.38e-07      |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.05e+05      |\n",
      "|    n_updates            | 1230          |\n",
      "|    policy_gradient_loss | -0.000301     |\n",
      "|    value_loss           | 8.25e+05      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 145          |\n",
      "|    ep_rew_mean          | -8.59e+03    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 645          |\n",
      "|    iterations           | 128          |\n",
      "|    time_elapsed         | 406          |\n",
      "|    total_timesteps      | 262144       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.666159e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -3.22        |\n",
      "|    explained_variance   | 5.96e-08     |\n",
      "|    learning_rate        | 1e-05        |\n",
      "|    loss                 | 4.89e+05     |\n",
      "|    n_updates            | 1270         |\n",
      "|    policy_gradient_loss | -0.000218    |\n",
      "|    value_loss           | 9.06e+05     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 150           |\n",
      "|    ep_rew_mean          | -8.88e+03     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 645           |\n",
      "|    iterations           | 132           |\n",
      "|    time_elapsed         | 418           |\n",
      "|    total_timesteps      | 270336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1195982e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -4.77e-07     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 4.08e+05      |\n",
      "|    n_updates            | 1310          |\n",
      "|    policy_gradient_loss | -0.000261     |\n",
      "|    value_loss           | 8.31e+05      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 150           |\n",
      "|    ep_rew_mean          | -8.8e+03      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 644           |\n",
      "|    iterations           | 136           |\n",
      "|    time_elapsed         | 432           |\n",
      "|    total_timesteps      | 278528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5249686e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -3.22         |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 1e-05         |\n",
      "|    loss                 | 3.92e+05      |\n",
      "|    n_updates            | 1350          |\n",
      "|    policy_gradient_loss | -0.000185     |\n",
      "|    value_loss           | 8.37e+05      |\n",
      "-------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "del env\n",
    "del model\n",
    "env = ArrayEnv3(5)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_5/\", learning_rate = 0.00001)\n",
    "\n",
    "#model = PPO.load(\"test4-01\")\n",
    "#model.set_env(env)\n",
    "\n",
    "model.learn(total_timesteps=10000000, log_interval=4)\n",
    "\n",
    "model.save(\"test5-03\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08419134-534b-4a5a-9ae7-bf9c7060ce5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "del env\n",
    "del model\n",
    "env = ArrayEnv2(5)\n",
    "env.reset()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1, tensorboard_log=\"./PPO_array_5/\", learning_rate = 0.000005)\n",
    "model.learn(total_timesteps=15000000, log_interval=4)\n",
    "\n",
    "model.save(\"test5-04\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1b092d-bc2f-4a76-a807-5c92bdc60371",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
