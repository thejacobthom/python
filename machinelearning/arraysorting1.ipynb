{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ddfd7da-da19-4e78-8879-d12f714210d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0988d66d-f2de-44b9-a41f-ea78981ed799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 48.6         |\n",
      "|    ep_rew_mean          | 48.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1103         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106117055 |\n",
      "|    clip_fraction        | 0.0966       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.638       |\n",
      "|    explained_variance   | 0.201        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0202      |\n",
      "|    value_loss           | 55           |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=4)\n",
    "model.save(\"dqn_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO.load(\"dqn_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "for x in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7654e32c-edeb-4fcf-a958-642b0b896ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be an environment of an array 4*1 (eg. [4,3,2,1] or [1,2,4,3]\n",
    "class ArrayEnv(Env):\n",
    "#git test\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.game_array_size = 100\n",
    "\n",
    "        self.action_space = Discrete(6) #6 possible types of swaps (01,02,03,12,13,23)\n",
    "\n",
    "        high = np.array([4] * 4)\n",
    "        low = np.array([0] * 4)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(4))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "        #print(self.state)\n",
    "        #get the two values to swap\n",
    "        if action == 0:\n",
    "            x_indice = 0\n",
    "            y_indice = 1\n",
    "            \n",
    "        elif action == 1:\n",
    "            x_indice = 0\n",
    "            y_indice = 2\n",
    "            \n",
    "        elif action == 2:\n",
    "            x_indice = 0\n",
    "            y_indice = 3\n",
    "            \n",
    "        elif action == 3:\n",
    "            x_indice = 1\n",
    "            y_indice = 2\n",
    "            \n",
    "        elif action == 4:\n",
    "            x_indice = 1\n",
    "            y_indice = 3\n",
    "            \n",
    "        elif action == 5:\n",
    "            x_indice = 2\n",
    "            y_indice = 3\n",
    "        else:\n",
    "            print(\"NO\")\n",
    "        \n",
    "\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/4)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(4))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa6b9474-5d34-4d1d-b17b-9c9a4ea19029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.5        |\n",
      "|    ep_rew_mean          | -679        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1161        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008183256 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 2.47e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.01e+04    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 2.12e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.21       |\n",
      "|    ep_rew_mean          | -135       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1078       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02213502 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 1.63e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.75e+04   |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    value_loss           | 6.72e+04   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.88         |\n",
      "|    ep_rew_mean          | -19.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1031         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070547266 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.719       |\n",
      "|    explained_variance   | 0.00616      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+03     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0193      |\n",
      "|    value_loss           | 5.6e+03      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.61        |\n",
      "|    ep_rew_mean          | -15.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1004        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007859043 |\n",
      "|    clip_fraction        | 0.0403      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.0035      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.2e+03     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 3.77e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.61        |\n",
      "|    ep_rew_mean          | -9.36       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 998         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013757024 |\n",
      "|    clip_fraction        | 0.0565      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.495      |\n",
      "|    explained_variance   | 0.023       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 514         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 1.22e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.65        |\n",
      "|    ep_rew_mean          | -7.33       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 999         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025895547 |\n",
      "|    clip_fraction        | 0.0928      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.428      |\n",
      "|    explained_variance   | 0.00373     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.74e+03    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.00378     |\n",
      "|    value_loss           | 1.63e+03    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = ArrayEnv()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=50000, log_interval=4)\n",
    "model.save(\"test4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48cf59b5-dc57-46d6-a3e4-c576a3ccdca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode:1 \tScore:1.55 \tMoves:2\n",
      "Episode:2 \tScore:1.55 \tMoves:2\n",
      "Episode:3 \tScore:1.875 \tMoves:3\n",
      "Episode:4 \tScore:1.55 \tMoves:2\n",
      "Episode:5 \tScore:-98.125 \tMoves:4\n",
      "Episode:6 \tScore:2.75 \tMoves:5\n",
      "Episode:7 \tScore:1.0 \tMoves:1\n",
      "Episode:8 \tScore:1.975 \tMoves:4\n",
      "Episode:9 \tScore:1.875 \tMoves:3\n",
      "Episode:10 \tScore:1.55 \tMoves:2\n",
      "Episode:11 \tScore:1.0 \tMoves:1\n",
      "Episode:12 \tScore:1.0 \tMoves:1\n",
      "Episode:13 \tScore:1.0 \tMoves:1\n",
      "Episode:14 \tScore:1.875 \tMoves:3\n",
      "Episode:15 \tScore:1.0 \tMoves:1\n",
      "Episode:16 \tScore:1.55 \tMoves:2\n",
      "Episode:17 \tScore:1.55 \tMoves:2\n",
      "Episode:18 \tScore:1.55 \tMoves:2\n",
      "Episode:19 \tScore:2.425 \tMoves:4\n",
      "Episode:20 \tScore:1.875 \tMoves:3\n",
      "Episode:21 \tScore:1.875 \tMoves:3\n",
      "Episode:22 \tScore:-97.9 \tMoves:4\n",
      "Episode:23 \tScore:1.975 \tMoves:4\n",
      "Episode:24 \tScore:1.875 \tMoves:3\n",
      "Episode:25 \tScore:1.0 \tMoves:1\n",
      "Episode:26 \tScore:2.75 \tMoves:5\n",
      "Episode:27 \tScore:1.875 \tMoves:3\n",
      "Episode:28 \tScore:1.875 \tMoves:3\n",
      "Episode:29 \tScore:1.55 \tMoves:2\n",
      "Episode:30 \tScore:1.875 \tMoves:3\n",
      "Episode:31 \tScore:1.65 \tMoves:3\n",
      "Episode:32 \tScore:1.55 \tMoves:2\n",
      "Episode:33 \tScore:1.0 \tMoves:1\n",
      "Episode:34 \tScore:1.55 \tMoves:2\n",
      "Episode:35 \tScore:-99.0 \tMoves:2\n",
      "Episode:36 \tScore:1.975 \tMoves:4\n",
      "Episode:37 \tScore:1.0 \tMoves:1\n",
      "Episode:38 \tScore:1.875 \tMoves:3\n",
      "Episode:39 \tScore:1.875 \tMoves:3\n",
      "Episode:40 \tScore:1.0 \tMoves:1\n",
      "Episode:41 \tScore:1.55 \tMoves:2\n",
      "Episode:42 \tScore:1.0 \tMoves:1\n",
      "Episode:43 \tScore:1.0 \tMoves:1\n",
      "Episode:44 \tScore:-99.0 \tMoves:2\n",
      "Episode:45 \tScore:1.55 \tMoves:2\n",
      "Episode:46 \tScore:1.0 \tMoves:1\n",
      "Episode:47 \tScore:1.55 \tMoves:2\n",
      "Episode:48 \tScore:1.55 \tMoves:2\n",
      "Episode:49 \tScore:1.875 \tMoves:3\n",
      "Episode:50 \tScore:1.875 \tMoves:3\n",
      "Episode:51 \tScore:1.875 \tMoves:3\n",
      "Episode:52 \tScore:1.975 \tMoves:4\n",
      "Episode:53 \tScore:1.55 \tMoves:2\n",
      "Episode:54 \tScore:1.0 \tMoves:1\n",
      "Episode:55 \tScore:-197.575 \tMoves:6\n",
      "Episode:56 \tScore:1.55 \tMoves:2\n",
      "Episode:57 \tScore:1.55 \tMoves:2\n",
      "Episode:58 \tScore:1.55 \tMoves:2\n",
      "Episode:59 \tScore:1.55 \tMoves:2\n",
      "Episode:60 \tScore:2.425 \tMoves:4\n",
      "Episode:61 \tScore:1.65 \tMoves:3\n",
      "Episode:62 \tScore:1.65 \tMoves:3\n",
      "Episode:63 \tScore:1.0 \tMoves:1\n",
      "Episode:64 \tScore:1.55 \tMoves:2\n",
      "Episode:65 \tScore:1.875 \tMoves:3\n",
      "Episode:66 \tScore:1.875 \tMoves:3\n",
      "Episode:67 \tScore:1.55 \tMoves:2\n",
      "Episode:68 \tScore:1.875 \tMoves:3\n",
      "Episode:69 \tScore:1.55 \tMoves:2\n",
      "Episode:70 \tScore:2.425 \tMoves:4\n",
      "Episode:71 \tScore:1.55 \tMoves:2\n",
      "Episode:72 \tScore:1.0 \tMoves:1\n",
      "Episode:73 \tScore:1.55 \tMoves:2\n",
      "Episode:74 \tScore:1.875 \tMoves:3\n",
      "Episode:75 \tScore:1.875 \tMoves:3\n",
      "Episode:76 \tScore:1.0 \tMoves:1\n",
      "Episode:77 \tScore:1.55 \tMoves:2\n",
      "Episode:78 \tScore:1.875 \tMoves:3\n",
      "Episode:79 \tScore:1.55 \tMoves:2\n",
      "Episode:80 \tScore:1.55 \tMoves:2\n",
      "Episode:81 \tScore:1.975 \tMoves:4\n",
      "Episode:82 \tScore:1.875 \tMoves:3\n",
      "Episode:83 \tScore:1.65 \tMoves:3\n",
      "Episode:84 \tScore:1.0 \tMoves:1\n",
      "Episode:85 \tScore:1.55 \tMoves:2\n",
      "Episode:86 \tScore:2.425 \tMoves:4\n",
      "Episode:87 \tScore:1.975 \tMoves:4\n",
      "Episode:88 \tScore:1.875 \tMoves:3\n",
      "Episode:89 \tScore:2.425 \tMoves:4\n",
      "Episode:90 \tScore:1.0 \tMoves:1\n",
      "Episode:91 \tScore:1.65 \tMoves:3\n",
      "Episode:92 \tScore:1.55 \tMoves:2\n",
      "Episode:93 \tScore:1.55 \tMoves:2\n",
      "Episode:94 \tScore:1.875 \tMoves:3\n",
      "Episode:95 \tScore:1.0 \tMoves:1\n",
      "Episode:96 \tScore:2.425 \tMoves:4\n",
      "Episode:97 \tScore:1.0 \tMoves:1\n",
      "Episode:98 \tScore:1.55 \tMoves:2\n",
      "Episode:99 \tScore:1.55 \tMoves:2\n",
      "Episode:100 \tScore:1.55 \tMoves:2\n"
     ]
    }
   ],
   "source": [
    "env = ArrayEnv()\n",
    "obs = env.reset()\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    #print(\"--- original array ---\")\n",
    "    #env.render()\n",
    "    #print(\"--- beginning sort ---\")\n",
    "    moves = 0\n",
    "    while not done:\n",
    "        moves+=1\n",
    "        action, _states = model.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        #env.render()\n",
    "    print(\"Episode:{} \\tScore:{} \\tMoves:{}\".format(episode, score, moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef491c0d-2a64-423a-a46f-2f26c11a808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ArrayEnv()\n",
    "for x in range(1000):\n",
    "    new_action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(new_action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"cool\")\n",
    "        obs = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852fb313-219f-4679-b397-f46e8eb44eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
