{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ddfd7da-da19-4e78-8879-d12f714210d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "from stable_baselines3 import PPO, DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0988d66d-f2de-44b9-a41f-ea78981ed799",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 48.6         |\n",
      "|    ep_rew_mean          | 48.6         |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1103         |\n",
      "|    iterations           | 4            |\n",
      "|    time_elapsed         | 7            |\n",
      "|    total_timesteps      | 8192         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0106117055 |\n",
      "|    clip_fraction        | 0.0966       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.638       |\n",
      "|    explained_variance   | 0.201        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 20.5         |\n",
      "|    n_updates            | 30           |\n",
      "|    policy_gradient_loss | -0.0202      |\n",
      "|    value_loss           | 55           |\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000, log_interval=4)\n",
    "model.save(\"dqn_cartpole\")\n",
    "\n",
    "del model # remove to demonstrate saving and loading\n",
    "\n",
    "model = PPO.load(\"dqn_cartpole\")\n",
    "\n",
    "obs = env.reset()\n",
    "for x in range(1000):\n",
    "    action, _states = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    env.render()\n",
    "    if done:\n",
    "      obs = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7654e32c-edeb-4fcf-a958-642b0b896ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this will be an environment of an array 4*1 (eg. [4,3,2,1] or [1,2,4,3]\n",
    "class ArrayEnv(Env):\n",
    "#git test\n",
    "    \n",
    "    def __init__(self):\n",
    "\n",
    "        self.game_array_size = 100\n",
    "\n",
    "        self.action_space = Discrete(6) #6 possible types of swaps (01,02,03,12,13,23)\n",
    "\n",
    "        high = np.array([4] * 4)\n",
    "        low = np.array([0] * 4)\n",
    "\n",
    "        self.observation_space = Box(low, high, dtype=np.int16)\n",
    "\n",
    "        #create an array of 100 random numbers between 0 and 1000\n",
    "        self.state = np.random.randint(1000, size=(4))\n",
    "        #game legnth of 100, shouldn't take more than 100 swaps\n",
    "\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "\n",
    "    #results when action taken\n",
    "    def step(self, action):\n",
    "        #print(self.state)\n",
    "        #get the two values to swap\n",
    "        if action == 0:\n",
    "            x_indice = 0\n",
    "            y_indice = 1\n",
    "            \n",
    "        elif action == 1:\n",
    "            x_indice = 0\n",
    "            y_indice = 2\n",
    "            \n",
    "        elif action == 2:\n",
    "            x_indice = 0\n",
    "            y_indice = 3\n",
    "            \n",
    "        elif action == 3:\n",
    "            x_indice = 1\n",
    "            y_indice = 2\n",
    "            \n",
    "        elif action == 4:\n",
    "            x_indice = 1\n",
    "            y_indice = 3\n",
    "            \n",
    "        elif action == 5:\n",
    "            x_indice = 2\n",
    "            y_indice = 3\n",
    "        else:\n",
    "            print(\"NO\")\n",
    "        \n",
    "\n",
    "        #save the original values in order for ease of reading\n",
    "        x_original = self.state[x_indice]\n",
    "        y_original = self.state[y_indice]\n",
    "\n",
    "        #perform the swap\n",
    "        temp = self.state[x_indice]\n",
    "        self.state[x_indice] = self.state[y_indice]\n",
    "        self.state[y_indice] = temp\n",
    "\n",
    "\n",
    "        #to calculate reward we first need to know how many elements are in the right spot\n",
    "        correct_position = np.count_nonzero(self.state == self.end_array)\n",
    "        # let's only reward if x comes before y in the array to simplify learning\n",
    "        if x_indice < y_indice:\n",
    "            #reward is set to the amount of things in correct position with size relative to\n",
    "            #0.9/100 so that when everything is in place, the reward == 0.9 and then may be added to if the\n",
    "            # movement itself is correct\n",
    "            reward = correct_position*(0.9/4)\n",
    "            #check if value at x is greater than value at y\n",
    "            if x_original > y_original:\n",
    "                #if a large x value is moving down the array\n",
    "                reward +=0.1\n",
    "            else:\n",
    "                #undesirable action i.e. swapping two equal values or moving a large value up in the array\n",
    "                reward = -100\n",
    "\n",
    "        #check if game is over by comparing the current state to the final intended array\n",
    "        if (self.state == self.end_array).all() == True:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "\n",
    "        #set placeholder for info\n",
    "        info = {}\n",
    "\n",
    "        #return all data\n",
    "        return self.state, reward, done, info        \n",
    "\n",
    "\n",
    "    #implement printing the array here\n",
    "    def render(self):\n",
    "        #print (np.count_nonzero(self.state == self.end_array))\n",
    "        print(self.state)\n",
    "\n",
    "    #reset/setup the environment\n",
    "    def reset(self):\n",
    "        #reset array to random numbers\n",
    "        self.state = np.random.randint(1000, size=(4))\n",
    "\n",
    "        #create a sorted array for our final state\n",
    "        self.end_array = np.copy(self.state)\n",
    "        self.end_array.sort()\n",
    "        #reset game length\n",
    "\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "aa6b9474-5d34-4d1d-b17b-9c9a4ea19029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 15.5        |\n",
      "|    ep_rew_mean          | -679        |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1161        |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 7           |\n",
      "|    total_timesteps      | 8192        |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008183256 |\n",
      "|    clip_fraction        | 0.0373      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -1.76       |\n",
      "|    explained_variance   | 2.47e-05    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.01e+04    |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.02       |\n",
      "|    value_loss           | 2.12e+05    |\n",
      "-----------------------------------------\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 5.21       |\n",
      "|    ep_rew_mean          | -135       |\n",
      "| time/                   |            |\n",
      "|    fps                  | 1078       |\n",
      "|    iterations           | 8          |\n",
      "|    time_elapsed         | 15         |\n",
      "|    total_timesteps      | 16384      |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 0.02213502 |\n",
      "|    clip_fraction        | 0.3        |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -1.49      |\n",
      "|    explained_variance   | 1.63e-05   |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 3.75e+04   |\n",
      "|    n_updates            | 70         |\n",
      "|    policy_gradient_loss | -0.0359    |\n",
      "|    value_loss           | 6.72e+04   |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.88         |\n",
      "|    ep_rew_mean          | -19.3        |\n",
      "| time/                   |              |\n",
      "|    fps                  | 1031         |\n",
      "|    iterations           | 12           |\n",
      "|    time_elapsed         | 23           |\n",
      "|    total_timesteps      | 24576        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 0.0070547266 |\n",
      "|    clip_fraction        | 0.0495       |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -0.719       |\n",
      "|    explained_variance   | 0.00616      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.44e+03     |\n",
      "|    n_updates            | 110          |\n",
      "|    policy_gradient_loss | -0.0193      |\n",
      "|    value_loss           | 5.6e+03      |\n",
      "------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.61        |\n",
      "|    ep_rew_mean          | -15.4       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 1004        |\n",
      "|    iterations           | 16          |\n",
      "|    time_elapsed         | 32          |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.007859043 |\n",
      "|    clip_fraction        | 0.0403      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.601      |\n",
      "|    explained_variance   | 0.0035      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.2e+03     |\n",
      "|    n_updates            | 150         |\n",
      "|    policy_gradient_loss | -0.0142     |\n",
      "|    value_loss           | 3.77e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.61        |\n",
      "|    ep_rew_mean          | -9.36       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 998         |\n",
      "|    iterations           | 20          |\n",
      "|    time_elapsed         | 41          |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.013757024 |\n",
      "|    clip_fraction        | 0.0565      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.495      |\n",
      "|    explained_variance   | 0.023       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 514         |\n",
      "|    n_updates            | 190         |\n",
      "|    policy_gradient_loss | -0.0136     |\n",
      "|    value_loss           | 1.22e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.65        |\n",
      "|    ep_rew_mean          | -7.33       |\n",
      "| time/                   |             |\n",
      "|    fps                  | 999         |\n",
      "|    iterations           | 24          |\n",
      "|    time_elapsed         | 49          |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.025895547 |\n",
      "|    clip_fraction        | 0.0928      |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -0.428      |\n",
      "|    explained_variance   | 0.00373     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 2.74e+03    |\n",
      "|    n_updates            | 230         |\n",
      "|    policy_gradient_loss | 0.00378     |\n",
      "|    value_loss           | 1.63e+03    |\n",
      "-----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "env = ArrayEnv()\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=50000, log_interval=4)\n",
    "model.save(\"test4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "48cf59b5-dc57-46d6-a3e4-c576a3ccdca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- original array ---\n",
      "[542 659  33 589]\n",
      "--- beginning sort ---\n",
      "Episode:1 Score:1.65\n",
      "--- original array ---\n",
      "[589 261 917   9]\n",
      "--- beginning sort ---\n",
      "Episode:2 Score:2.425\n",
      "--- original array ---\n",
      "[216 340  96 279]\n",
      "--- beginning sort ---\n",
      "Episode:3 Score:1.65\n",
      "--- original array ---\n",
      "[945 281 507 651]\n",
      "--- beginning sort ---\n",
      "Episode:4 Score:1.875\n",
      "--- original array ---\n",
      "[501   4 678 628]\n",
      "--- beginning sort ---\n",
      "Episode:5 Score:1.55\n",
      "--- original array ---\n",
      "[195 339 278 142]\n",
      "--- beginning sort ---\n",
      "Episode:6 Score:1.75\n",
      "--- original array ---\n",
      "[394 632 150 312]\n",
      "--- beginning sort ---\n",
      "Episode:7 Score:1.55\n",
      "--- original array ---\n",
      "[248 214 169 688]\n",
      "--- beginning sort ---\n",
      "Episode:8 Score:1.875\n",
      "--- original array ---\n",
      "[911 817 275 849]\n",
      "--- beginning sort ---\n",
      "Episode:9 Score:1.55\n",
      "--- original array ---\n",
      "[239 935 870 472]\n",
      "--- beginning sort ---\n",
      "Episode:10 Score:1.875\n",
      "--- original array ---\n",
      "[606 265  81 847]\n",
      "--- beginning sort ---\n",
      "Episode:11 Score:1.0\n",
      "--- original array ---\n",
      "[622 692  61 365]\n",
      "--- beginning sort ---\n",
      "Episode:12 Score:1.975\n",
      "--- original array ---\n",
      "[737 226 927 344]\n",
      "--- beginning sort ---\n",
      "Episode:13 Score:1.875\n",
      "--- original array ---\n",
      "[ 72 645 539 306]\n",
      "--- beginning sort ---\n",
      "Episode:14 Score:1.0\n",
      "--- original array ---\n",
      "[654 970 328 719]\n",
      "--- beginning sort ---\n",
      "Episode:15 Score:1.65\n",
      "--- original array ---\n",
      "[687  78 743 367]\n",
      "--- beginning sort ---\n",
      "Episode:16 Score:1.875\n",
      "--- original array ---\n",
      "[293 116 797 525]\n",
      "--- beginning sort ---\n",
      "Episode:17 Score:1.55\n",
      "--- original array ---\n",
      "[632 311 930 640]\n",
      "--- beginning sort ---\n",
      "Episode:18 Score:1.55\n",
      "--- original array ---\n",
      "[ 97 956 844 466]\n",
      "--- beginning sort ---\n",
      "Episode:19 Score:1.0\n",
      "--- original array ---\n",
      "[772 886 683 996]\n",
      "--- beginning sort ---\n",
      "Episode:20 Score:1.55\n",
      "--- original array ---\n",
      "[998 346   0  48]\n",
      "--- beginning sort ---\n",
      "Episode:21 Score:1.65\n",
      "--- original array ---\n",
      "[274 330 596 291]\n",
      "--- beginning sort ---\n",
      "Episode:22 Score:1.55\n",
      "--- original array ---\n",
      "[430  98 411 114]\n",
      "--- beginning sort ---\n",
      "Episode:23 Score:1.55\n",
      "--- original array ---\n",
      "[ 87 696 223   0]\n",
      "--- beginning sort ---\n",
      "Episode:24 Score:1.55\n",
      "--- original array ---\n",
      "[115 854 353   6]\n",
      "--- beginning sort ---\n",
      "Episode:25 Score:1.55\n",
      "--- original array ---\n",
      "[663 379 511 620]\n",
      "--- beginning sort ---\n",
      "Episode:26 Score:1.875\n",
      "--- original array ---\n",
      "[867  88 898 932]\n",
      "--- beginning sort ---\n",
      "Episode:27 Score:1.0\n",
      "--- original array ---\n",
      "[524 313 215 756]\n",
      "--- beginning sort ---\n",
      "Episode:28 Score:1.875\n",
      "--- original array ---\n",
      "[278 356 429 649]\n",
      "--- beginning sort ---\n",
      "Episode:29 Score:-99.0\n",
      "--- original array ---\n",
      "[876  25   3 607]\n",
      "--- beginning sort ---\n",
      "Episode:30 Score:1.55\n",
      "--- original array ---\n",
      "[506 911 742 632]\n",
      "--- beginning sort ---\n",
      "Episode:31 Score:1.0\n",
      "--- original array ---\n",
      "[795 697 826  35]\n",
      "--- beginning sort ---\n",
      "Episode:32 Score:1.55\n",
      "--- original array ---\n",
      "[ 24 622 314 320]\n",
      "--- beginning sort ---\n",
      "Episode:33 Score:1.55\n",
      "--- original array ---\n",
      "[407 220 978 441]\n",
      "--- beginning sort ---\n",
      "Episode:34 Score:1.55\n",
      "--- original array ---\n",
      "[ 16 633 301 508]\n",
      "--- beginning sort ---\n",
      "Episode:35 Score:1.55\n",
      "--- original array ---\n",
      "[562 985  65 308]\n",
      "--- beginning sort ---\n",
      "Episode:36 Score:1.975\n",
      "--- original array ---\n",
      "[799 280 767 378]\n",
      "--- beginning sort ---\n",
      "Episode:37 Score:1.975\n",
      "--- original array ---\n",
      "[260 909 946 702]\n",
      "--- beginning sort ---\n",
      "Episode:38 Score:1.55\n",
      "--- original array ---\n",
      "[636 538 538  25]\n",
      "--- beginning sort ---\n",
      "Episode:39 Score:1.875\n",
      "--- original array ---\n",
      "[555 631 866 670]\n",
      "--- beginning sort ---\n",
      "Episode:40 Score:1.0\n",
      "--- original array ---\n",
      "[159  52 189 689]\n",
      "--- beginning sort ---\n",
      "Episode:41 Score:1.0\n",
      "--- original array ---\n",
      "[515 297 760 859]\n",
      "--- beginning sort ---\n",
      "Episode:42 Score:1.0\n",
      "--- original array ---\n",
      "[553 674 308 660]\n",
      "--- beginning sort ---\n",
      "Episode:43 Score:1.65\n",
      "--- original array ---\n",
      "[ 87 484 265 355]\n",
      "--- beginning sort ---\n",
      "Episode:44 Score:1.55\n",
      "--- original array ---\n",
      "[817 572 890 885]\n",
      "--- beginning sort ---\n",
      "Episode:45 Score:-97.9\n",
      "--- original array ---\n",
      "[155  23 322 796]\n",
      "--- beginning sort ---\n",
      "Episode:46 Score:1.0\n",
      "--- original array ---\n",
      "[ 54 429  24 890]\n",
      "--- beginning sort ---\n",
      "Episode:47 Score:1.55\n",
      "--- original array ---\n",
      "[447 112 132 302]\n",
      "--- beginning sort ---\n",
      "Episode:48 Score:1.875\n",
      "--- original array ---\n",
      "[579 989 958 779]\n",
      "--- beginning sort ---\n",
      "Episode:49 Score:1.875\n",
      "--- original array ---\n",
      "[280 579 778 524]\n",
      "--- beginning sort ---\n",
      "Episode:50 Score:1.55\n",
      "--- original array ---\n",
      "[ 44 290 833 976]\n",
      "--- beginning sort ---\n",
      "Episode:51 Score:-99.0\n",
      "--- original array ---\n",
      "[460 502 879 516]\n",
      "--- beginning sort ---\n",
      "Episode:52 Score:1.0\n",
      "--- original array ---\n",
      "[639 724 585 792]\n",
      "--- beginning sort ---\n",
      "Episode:53 Score:1.55\n",
      "--- original array ---\n",
      "[122 229 813 939]\n",
      "--- beginning sort ---\n",
      "Episode:54 Score:-99.0\n",
      "--- original array ---\n",
      "[665 100 687 229]\n",
      "--- beginning sort ---\n",
      "Episode:55 Score:1.875\n",
      "--- original array ---\n",
      "[544 144 163 390]\n",
      "--- beginning sort ---\n",
      "Episode:56 Score:1.875\n",
      "--- original array ---\n",
      "[872 407 961 591]\n",
      "--- beginning sort ---\n",
      "Episode:57 Score:1.875\n",
      "--- original array ---\n",
      "[142  68 269 328]\n",
      "--- beginning sort ---\n",
      "Episode:58 Score:1.0\n",
      "--- original array ---\n",
      "[800  87 841 432]\n",
      "--- beginning sort ---\n",
      "Episode:59 Score:1.875\n",
      "--- original array ---\n",
      "[452 854 635 487]\n",
      "--- beginning sort ---\n",
      "Episode:60 Score:1.0\n",
      "--- original array ---\n",
      "[844  57 675 651]\n",
      "--- beginning sort ---\n",
      "Episode:61 Score:1.55\n",
      "--- original array ---\n",
      "[237 299 415 934]\n",
      "--- beginning sort ---\n",
      "Episode:62 Score:-99.0\n",
      "--- original array ---\n",
      "[308 982 490 264]\n",
      "--- beginning sort ---\n",
      "Episode:63 Score:1.55\n",
      "--- original array ---\n",
      "[ 49  25 560  95]\n",
      "--- beginning sort ---\n",
      "Episode:64 Score:1.55\n",
      "--- original array ---\n",
      "[296 532 324 331]\n",
      "--- beginning sort ---\n",
      "Episode:65 Score:1.55\n",
      "--- original array ---\n",
      "[350 493 635 704]\n",
      "--- beginning sort ---\n",
      "Episode:66 Score:-99.0\n",
      "--- original array ---\n",
      "[399 178 275  90]\n",
      "--- beginning sort ---\n",
      "Episode:67 Score:1.0\n",
      "--- original array ---\n",
      "[649 405 904 347]\n",
      "--- beginning sort ---\n",
      "Episode:68 Score:1.55\n",
      "--- original array ---\n",
      "[711 203 700 116]\n",
      "--- beginning sort ---\n",
      "Episode:69 Score:2.075\n",
      "--- original array ---\n",
      "[932 661 257 700]\n",
      "--- beginning sort ---\n",
      "Episode:70 Score:1.55\n",
      "--- original array ---\n",
      "[209 973 846 977]\n",
      "--- beginning sort ---\n",
      "Episode:71 Score:1.0\n",
      "--- original array ---\n",
      "[217 768 772 921]\n",
      "--- beginning sort ---\n",
      "Episode:72 Score:-99.0\n",
      "--- original array ---\n",
      "[654 624 563 376]\n",
      "--- beginning sort ---\n",
      "Episode:73 Score:2.175\n",
      "--- original array ---\n",
      "[ 99 472 894 543]\n",
      "--- beginning sort ---\n",
      "Episode:74 Score:1.0\n",
      "--- original array ---\n",
      "[603 379  40 280]\n",
      "--- beginning sort ---\n",
      "Episode:75 Score:1.875\n",
      "--- original array ---\n",
      "[618 962  22 971]\n",
      "--- beginning sort ---\n",
      "Episode:76 Score:1.55\n",
      "--- original array ---\n",
      "[393 286 393 738]\n",
      "--- beginning sort ---\n",
      "Episode:77 Score:1.0\n",
      "--- original array ---\n",
      "[227 860 346 792]\n",
      "--- beginning sort ---\n",
      "Episode:78 Score:1.55\n",
      "--- original array ---\n",
      "[  1 834 723 305]\n",
      "--- beginning sort ---\n",
      "Episode:79 Score:1.875\n",
      "--- original array ---\n",
      "[549 340 958 676]\n",
      "--- beginning sort ---\n",
      "Episode:80 Score:1.55\n",
      "--- original array ---\n",
      "[322 757 697 894]\n",
      "--- beginning sort ---\n",
      "Episode:81 Score:1.0\n",
      "--- original array ---\n",
      "[832 922 444 565]\n",
      "--- beginning sort ---\n",
      "Episode:82 Score:1.975\n",
      "--- original array ---\n",
      "[986 246 112 418]\n",
      "--- beginning sort ---\n",
      "Episode:83 Score:1.55\n",
      "--- original array ---\n",
      "[265 986 500 325]\n",
      "--- beginning sort ---\n",
      "Episode:84 Score:1.875\n",
      "--- original array ---\n",
      "[670 259 178 776]\n",
      "--- beginning sort ---\n",
      "Episode:85 Score:1.875\n",
      "--- original array ---\n",
      "[122  83 648  81]\n",
      "--- beginning sort ---\n",
      "Episode:86 Score:2.425\n",
      "--- original array ---\n",
      "[976 615 767 383]\n",
      "--- beginning sort ---\n",
      "Episode:87 Score:1.0\n",
      "--- original array ---\n",
      "[943  14 365 189]\n",
      "--- beginning sort ---\n",
      "Episode:88 Score:1.55\n",
      "--- original array ---\n",
      "[416 201 575 159]\n",
      "--- beginning sort ---\n",
      "Episode:89 Score:2.425\n",
      "--- original array ---\n",
      "[751 868 915 749]\n",
      "--- beginning sort ---\n",
      "Episode:90 Score:1.875\n",
      "--- original array ---\n",
      "[299 562  67 441]\n",
      "--- beginning sort ---\n",
      "Episode:91 Score:1.875\n",
      "--- original array ---\n",
      "[299 126 601 316]\n",
      "--- beginning sort ---\n",
      "Episode:92 Score:1.55\n",
      "--- original array ---\n",
      "[456 724 520 668]\n",
      "--- beginning sort ---\n",
      "Episode:93 Score:-97.9\n",
      "--- original array ---\n",
      "[176 761 683 161]\n",
      "--- beginning sort ---\n",
      "Episode:94 Score:1.75\n",
      "--- original array ---\n",
      "[796  99 841 129]\n",
      "--- beginning sort ---\n",
      "Episode:95 Score:-97.8\n",
      "--- original array ---\n",
      "[787 327 931 158]\n",
      "--- beginning sort ---\n",
      "Episode:96 Score:2.425\n",
      "--- original array ---\n",
      "[519 692 871 735]\n",
      "--- beginning sort ---\n",
      "Episode:97 Score:1.0\n",
      "--- original array ---\n",
      "[901 819  11  28]\n",
      "--- beginning sort ---\n",
      "Episode:98 Score:1.875\n",
      "--- original array ---\n",
      "[883 352 745 116]\n",
      "--- beginning sort ---\n",
      "Episode:99 Score:1.0\n",
      "--- original array ---\n",
      "[788  27 615 682]\n",
      "--- beginning sort ---\n",
      "Episode:100 Score:1.875\n"
     ]
    }
   ],
   "source": [
    "env = ArrayEnv()\n",
    "obs = env.reset()\n",
    "\n",
    "episodes = 100\n",
    "for episode in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    score = 0 \n",
    "    #print(\"--- original array ---\")\n",
    "    #env.render()\n",
    "    #print(\"--- beginning sort ---\")\n",
    "    moves = 0\n",
    "    while not done:\n",
    "        moves+=1\n",
    "        action, _states = model.predict(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score+=reward\n",
    "        #env.render()\n",
    "    print(\"Episode:{} \\tScore:{} \\tMoves:{}\".format(episode, score, moves))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef491c0d-2a64-423a-a46f-2f26c11a808f",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ArrayEnv()\n",
    "for x in range(1000):\n",
    "    new_action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(new_action)\n",
    "    env.render()\n",
    "    if done:\n",
    "        print(\"cool\")\n",
    "        obs = env.reset()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852fb313-219f-4679-b397-f46e8eb44eca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
